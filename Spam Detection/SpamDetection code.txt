import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

mushroom=pd.read_csv("/kaggle/input/datasetabhi/mushrooms.csv")

mushroom.head()

mushroom.tail()

mushroom.shape

le = LabelEncoder()

ds = mushroom.apply(func=le.fit_transform)

ds.head()

data = ds.values
X = data[:, 1:]
y = data[:, 0]

X.shape, y.shape

X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, random_state=42)


*Custom NB*

class CustomNB:
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
        
    # label = which class you want this prob for
    def prior_prob(self, label): 
        total = self.y_train.shape[0]
        class_examples = np.sum(self.y_train == label)
        return class_examples / float(total)  # python 2
    
    # P(Xi=red|y=label) - ith feature (feature col = i) for a single example
    def conditional_prob(self, feature_col, feature_val, label):
        # out of all the examples, what mushrooms have feature as feature_val in the feature_col that belongs to that class label
        X_filtered = self.X_train[self.y_train==label] # all the examples in class label
        numerator = np.sum(X_filtered[:, feature_col] == feature_val)
        denominator = len(X_filtered)
        return numerator / denominator
    
    # we are going to do this for all the 22 features that we have for each example
    def predict_point(self, X_test):
        # X_test is a single example with n features
        classes = np.unique(self.y_train) # By default from 0
        n_features = self.X_train.shape[1]
        post_pro = []
        # post prob for each class
        for label in classes:
            # post_prob = prior * likelihood
            likehood = 1.0
            for feature in range(n_features):
                cond = self.conditional_prob(feature, X_test[feature], label)
                likehood *= cond
            prior = self.prior_prob(label)
            post = prior * likehood
            post_pro.append(post)
        
        # ans = max value from all labels
        return np.argmax(post_pro) # return the index of the largest value in array
    
    def predict(self, X_test):
        result = []
        for point in X_test:
            result.append(self.predict_point(point))
        return np.array(result)
    
    def score(self, X_test, y_test):
        return (self.predict(X_test) == y_test).mean()

model = CustomNB()

model.fit(X_train, y_train)

model.predict(X_test[:10])

y_test[:10]

model.score(X_test, y_test)*100

*NTLK*

import nltk

from nltk.corpus import brown

brown.categories()

brown.words()

data = brown.sents(categories=["adventure"])

" ".join(data[9])

*Tokenization*

from nltk.tokenize import sent_tokenize, word_tokenize

document = """ It was a very good movie. The cast was amazing and I liked the story.
I went to the movie hall to see it.
"""

sentence = "Code for Cause is too OP kunal@codeforcause.org"

sents = sent_tokenize(document)
print(sents)
len(sents)

words = word_tokenize(sentence) # also break down special characters
print(words)
print(len(words))

*Stopword removal*

from nltk.corpus import stopwords

sw = set(stopwords.words('english'))

text = "i am not a very good cricket player".split()
print(text)

def remove_stoprwords(text, stopwords):
    useful = [w for w in text if w not in stopwords]
    return useful

useful_words = remove_stoprwords(text, sw)

useful_words

sent = "My email is kunal@codeforcause.org, please don't spam my inbox"

from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer('[a-zA-Z@.]+')
useful = tokenizer.tokenize(sentence)
print(useful)

from nltk.stem import SnowballStemmer, PorterStemmer, LancasterStemmer

ps = PorterStemmer()

ps.stem('laughing')

corpus = [
    'Dan Morgan told himself he would forget Ann Turner.',
    'Sometimes he woke up in the middle of the night thinking of Ann , and then could not get back to sleep .',
    'His plans and dreams had revolved around her so much and for so long that now he felt as if he had nothing .',
    'He found that if he was tired enough at night , he went to sleep simply because he was too exhausted to stay awake .'
]

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()

vc = cv.fit_transform(corpus)

vc = vc.toarray()
print(vc)
print(cv.vocabulary_)

print(len(cv.vocabulary_))

numbers = vc[2]

print(len(numbers))

len(vc[1])

def myTokenizer(document):
    words = tokenizer.tokenize(document.lower())
    # remove the stopwords
    words = remove_stoprwords(words, sw)
    return words

myTokenizer('this is a random text')

cv = CountVectorizer(tokenizer=myTokenizer)

vc = cv.fit_transform(corpus).toarray()

print(vc)

len(vc[0])

cv.vocabulary_

len(cv.transform([sent]).toarray()[0])

cv.vocabulary_

*SpamDetection*

from nltk.tokenize import RegexpTokenizer, word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

df = pd.read_csv('/kaggle/input/datasetabhi/spam.csv', encoding='ISO-8859-1')
le = LabelEncoder()

data = df.to_numpy()

X = data[:, 1]
y = data[:, 0]

X.shape, y.shape

tokenizer = RegexpTokenizer('\w+')
sw = set(stopwords.words('english'))
ps = PorterStemmer()

def getStem(review):
    review = review.lower()
    tokens = tokenizer.tokenize(review) # breaking into small words
    removed_stopwords = [w for w in tokens if w not in sw]
    stemmed_words = [ps.stem(token) for token in removed_stopwords]
    clean_review = ' '.join(stemmed_words)
    return clean_review

def getDoc(document):
    d = []
    for doc in document:
        d.append(getStem(doc))
    return d

stemmed_doc = getDoc(X)

stemmed_doc[:10]

cv = CountVectorizer()

vc = cv.fit_transform(stemmed_doc)

X = vc.todense()

X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, random_state=42)

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)
model.score(X_train, y_train)*100
model.score(X_test,y_test)*100

messages = [
    """
    Hi Kunal,
We invite you to participate in MishMash - Indiaâ€™s largest online diversity hackathon. 
The hackathon is a Skillenza initiative and sponsored by Microsoft, Unity, Unilever, Gojek, Rocketium and Jharkhand Government. 
We have a special theme for you - Deep Tech/Machine Learning - sponsored by Unilever, which will be perfect for you.
    """,
    """Join us today at 12:00 PM ET / 16:00 UTC for a Red Hat DevNation tech talk on AWS Lambda and serverless Java with Bill Burke.
Have you ever tried Java on AWS Lambda but found that the cold-start latency and memory usage were far too high? 
In this session, we will show how we optimized Java for serverless applications by leveraging GraalVM with Quarkus to 
provide both supersonic startup speed and a subatomic memory footprint.""",

    """We really appreciate your interest and wanted to let you know that we have received your application.
There is strong competition for jobs at Intel, and we receive many applications. As a result, it may take some time to get back to you.
Whether or not this position ends up being a fit, we will keep your information per data retention policies, 
so we can contact you for other positions that align to your experience and skill set.
"""
]

def prepare(messages):
    d = getDoc(messages)
    # dont do fit_transform!! it will create new vocab.
    return cv.transform(d)

messages = prepare(messages)

y_pred = model.predict(messages)
y_pred

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn.score(X_train, y_train)*100
knn.score(X_test, y_test)*100